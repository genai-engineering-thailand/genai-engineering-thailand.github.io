{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#join-our-community","title":"Join our community","text":"<p>Let's connect and make our community stay alive. Please read our contribution guidelines about detailed workflow. For general questions or updates, public discussions and event appointments, please join our community.</p> <p> </p> <p>All contributions are greatly appreciated </p>"},{"location":"awesome_curation/","title":"Content Curations","text":""},{"location":"awesome_curation/#awesome-curations","title":"Awesome Curations","text":"<p>The following list consists of awesome portals related to LLM and Generative AI in various aspects.</p> <ul> <li>Awesome-LLM</li> <li>LLMsPracticalGuide</li> </ul>"},{"location":"knowledge-sharing/content_index/","title":"Content Index","text":""},{"location":"knowledge-sharing/content_index/#knowledge-sharing","title":"Knowledge Sharing","text":"Date Knowledge Content 17 May 2024 Self-hosted LLM on GCP 7 June 2024 PDF-to-Text: A nightmare that never ends 10 June 2024 Introduction to Ollama 23 June 2024 Build AI App with Vercel SDK 28 June 2024 Monitoring and Observability in LLM Application 6 July 2024 Optimizing LLM with LoRA"},{"location":"knowledge-sharing/0%20-%20Self-hosted%20LLM%20on%20GCP/","title":"Self-hosted LLM on GCP","text":""},{"location":"knowledge-sharing/0%20-%20Self-hosted%20LLM%20on%20GCP/#resource","title":"Resource","text":"<p>Video | Material | Knowledge Summary</p>"},{"location":"knowledge-sharing/0%20-%20Self-hosted%20LLM%20on%20GCP/#summary","title":"Summary","text":"<p>The following content is summarized by Gemini.</p> <p>Topic 1: Introduction</p> <ul> <li>This is the first event hosted by GenAI Engineer Thailand, a community for people who are interested in generative AI.</li> <li>The goal of this event is to share knowledge and learn from each other.</li> <li>The speaker for this event is Mr. Coco - Senior AI/ML Engineer from ArcFusion.ai</li> </ul> <p>Topic 2: LLM deployment on GCP</p> <ul> <li>Large Language Model (LLM) inference is memory bound, not compute bound. This means it takes longer to load data into GPU memory than it does to process the data itself. </li> <li>Because of this, the bottleneck for LLM inference is the size of the model that can fit into GPU memory.</li> <li>Techniques to reduce memory usage and speed up inference include quantization, like AutoGPTQ (reducing the representation size of data) or FlashAttention (modifying the Attention mechanism).</li> <li>Ray Serve and vLLM are frameworks that can be used for LLM deployment. </li> <li>Ray and vLLM are tools used together to run large language models (LLMs) efficiently:<ul> <li>Ray:<ul> <li>Distributed computing framework</li> <li>Manages resources across multiple machines</li> <li>Enables parallel processing and scaling</li> </ul> </li> <li>vLLM:<ul> <li>Open-source LLM inference engine</li> <li>Optimizes memory usage for faster inference</li> </ul> </li> </ul> </li> </ul> <p>Together, Ray and vLLM provide a powerful solution for deploying and scaling LLMs in production environments. They are often used to build high-performance LLM serving systems that can handle large volumes of requests.</p>"},{"location":"knowledge-sharing/0%20-%20Self-hosted%20LLM%20on%20GCP/material/content/","title":"content","text":""},{"location":"knowledge-sharing/0%20-%20Self-hosted%20LLM%20on%20GCP/material/content/#files","title":"Files","text":"<ul> <li>PDF</li> </ul>"},{"location":"knowledge-sharing/1%20-%20PDF-to-Text%20-%20A%20nightmare%20that%20never%20ends/","title":"PDF-to-Text: A nightmare that never ends","text":""},{"location":"knowledge-sharing/1%20-%20PDF-to-Text%20-%20A%20nightmare%20that%20never%20ends/#resource","title":"Resource","text":"<p>Video | Material | Knowledge Summary</p>"},{"location":"knowledge-sharing/1%20-%20PDF-to-Text%20-%20A%20nightmare%20that%20never%20ends/#summary","title":"Summary","text":"<p>The video is about PDF-to-text and the challenges involved. The speaker, Napat, an AI/ML engineer at ArcFusion.ai who has experience working on PDF-to-Text conversion, discusses the difficulties of converting PDFs to text, especially scanned PDFs.</p> <p>The speaker mentions that there are 3 main components to consider when working with PDFs: images, tables, and text.</p> <ul> <li>Images: Extracting images from digital PDFs is straightforward, but extracting images from scanned PDFs requires using object detection.</li> <li>Tables: Extracting tables can be done using libraries if the tables have borders around each cell. Extracting tables without borders is more challenging.</li> <li>Text: Text extraction is the most important part. The speaker mentions that there are challenges with accuracy, especially when dealing with scanned PDFs, which needed OCR.</li> </ul>"},{"location":"knowledge-sharing/1%20-%20PDF-to-Text%20-%20A%20nightmare%20that%20never%20ends/material/content/","title":"content","text":""},{"location":"knowledge-sharing/1%20-%20PDF-to-Text%20-%20A%20nightmare%20that%20never%20ends/material/content/#files","title":"Files","text":"<ul> <li>PDF</li> </ul>"},{"location":"knowledge-sharing/2%20-%20Introduction%20to%20Ollama/","title":"Introduction to Ollama","text":""},{"location":"knowledge-sharing/2%20-%20Introduction%20to%20Ollama/#resource","title":"Resource","text":"<p>Video | Material | Knowledge Summary</p>"},{"location":"knowledge-sharing/2%20-%20Introduction%20to%20Ollama/#summary","title":"Summary","text":"<p>The following content is summarized by Gemini.</p> <p>Introduction</p> <p>The speaker introduces himself as Pako, a Tech guy from PALO IT. He is excited to share what he learned about Ollama.</p> <p>Why run LLM or SLM locally:</p> <p>The speaker discusses why one would want to run LLM or SLM locally when there are already commercial, well-known servers available. Reasons for running locally include:</p> <ul> <li>Developer can learn and experiment with LM concepts more easily on their own machine without any cost.</li> <li>It helps improve coding skills in writing prompts as the local LM is not as powerful as mainstream models.</li> </ul> <p>What is Ollama:</p> <p>Ollama is a wrapper for llama.cpp. It allows you to run LLM on your local CPU or laptop. Ollama converts the model to a format that can be run on the CPU. It is a powerful tool that can run various things including converting models. Ollama improves developer experience by allowing them to run LLM and serve as an APIs. This means you can use large language models through APIs. Ollama becomes versatile and can be used in Docker, Kubernetes environments, etc. It can also be called through NodeJS, Python, etc. Ollama comes with good community support on Discord.</p> <p>Background of Ollama:</p> <p>The speaker talks about the background of Ollama. Since the developers of Ollama previously worked on Docker, the command line interface (CLI) is similar to Docker. For instance, commands like push model, pull model, and ama run are similar to Docker commands.</p> <p>How to use OLM:</p> <p>Ollama is not necessarily the fastest because it runs on your local machine. Be patient when running models as it depends on your CPU. Ollama itself is not a model, it runs models that are converted to gguf format. Converting models to gguf is not difficult and there are tools available including Ollama itself. There are instructions and libraries to convert various model formats to gguf.</p> <p>The speaker walks through downloading and running Ollama from the Github page. He shows that Ollama can list models available on the machine and you can see the configuration by running the show config command. Ollama also has templates which are ready-made prompts that can be used. It can also run LoRA adapters.</p> <p>What's Next</p> <p>The speaker suggests exploring Kubernetes with Ollama to run models on Kubernetes. He also recommends exploring creating custom models and using Ollama with libraries like Tensorflow or PyTorch. Finally, he suggests trying out fine-tuning models from UnSloth and using them in Ollama.</p>"},{"location":"knowledge-sharing/2%20-%20Introduction%20to%20Ollama/material/content/","title":"content","text":""},{"location":"knowledge-sharing/2%20-%20Introduction%20to%20Ollama/material/content/#files","title":"Files","text":"<ul> <li>PDF</li> </ul>"},{"location":"knowledge-sharing/3%20-%20Build%20AI%20App%20with%20Vercel%20SDK/","title":"Build AI App with Vercel SDK","text":""},{"location":"knowledge-sharing/3%20-%20Build%20AI%20App%20with%20Vercel%20SDK/#resource","title":"Resource","text":"<p>Video | Material | Knowledge Summary</p>"},{"location":"knowledge-sharing/3%20-%20Build%20AI%20App%20with%20Vercel%20SDK/#summary","title":"Summary","text":"<p>Check out knowledge summary.</p>"},{"location":"knowledge-sharing/3%20-%20Build%20AI%20App%20with%20Vercel%20SDK/material/content/","title":"content","text":""},{"location":"knowledge-sharing/3%20-%20Build%20AI%20App%20with%20Vercel%20SDK/material/content/#files","title":"Files","text":"<ul> <li>Deck</li> <li>Code from  original repo</li> </ul>"},{"location":"knowledge-sharing/3%20-%20Build%20AI%20App%20with%20Vercel%20SDK/material/vercel-ai-sdk-talk/","title":"README","text":""},{"location":"knowledge-sharing/3%20-%20Build%20AI%20App%20with%20Vercel%20SDK/material/vercel-ai-sdk-talk/#this-git-repo-is-the-code-example-of-vercel-ai-sdk-talk-at-genai-engineer-thailand-session-open-in-github","title":"This Git repo is the code example of Vercel AI SDK Talk at GenAI Engineer Thailand session (open in github)","text":"<p>Rerun Video https://www.youtube.com/watch?v=WmH4BSZwqqE</p> <p>Summary https://txt.lukkiddd.com/build-ai-app-with-vercel-sdk/</p>"},{"location":"knowledge-sharing/3%20-%20Build%20AI%20App%20with%20Vercel%20SDK/material/vercel-ai-sdk-talk/nextjs/","title":"README","text":"<p>This is a Next.js project bootstrapped with <code>create-next-app</code>.</p>"},{"location":"knowledge-sharing/3%20-%20Build%20AI%20App%20with%20Vercel%20SDK/material/vercel-ai-sdk-talk/nextjs/#getting-started","title":"Getting Started","text":"<p>First, run the development server:</p> <pre><code>npm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n</code></pre> <p>Open http://localhost:3000 with your browser to see the result.</p> <p>You can start editing the page by modifying <code>app/page.tsx</code>. The page auto-updates as you edit the file.</p> <p>This project uses <code>next/font</code> to automatically optimize and load Inter, a custom Google Font.</p>"},{"location":"knowledge-sharing/3%20-%20Build%20AI%20App%20with%20Vercel%20SDK/material/vercel-ai-sdk-talk/nextjs/#learn-more","title":"Learn More","text":"<p>To learn more about Next.js, take a look at the following resources:</p> <ul> <li>Next.js Documentation - learn about Next.js features and API.</li> <li>Learn Next.js - an interactive Next.js tutorial.</li> </ul> <p>You can check out the Next.js GitHub repository - your feedback and contributions are welcome!</p>"},{"location":"knowledge-sharing/3%20-%20Build%20AI%20App%20with%20Vercel%20SDK/material/vercel-ai-sdk-talk/nextjs/#deploy-on-vercel","title":"Deploy on Vercel","text":"<p>The easiest way to deploy your Next.js app is to use the Vercel Platform from the creators of Next.js.</p> <p>Check out our Next.js deployment documentation for more details.</p>"},{"location":"knowledge-sharing/4%20-%20Monitoring%20and%20Observability%20in%20LLM%20Application/","title":"Monitoring and Observability in LLM Application","text":""},{"location":"knowledge-sharing/4%20-%20Monitoring%20and%20Observability%20in%20LLM%20Application/#resource","title":"Resource","text":"<p>Video | Material | Knowledge Summary</p>"},{"location":"knowledge-sharing/4%20-%20Monitoring%20and%20Observability%20in%20LLM%20Application/#summary","title":"Summary","text":"<p>Check out knowledge summary.</p>"},{"location":"knowledge-sharing/4%20-%20Monitoring%20and%20Observability%20in%20LLM%20Application/material/content/","title":"content","text":""},{"location":"knowledge-sharing/4%20-%20Monitoring%20and%20Observability%20in%20LLM%20Application/material/content/#files","title":"Files","text":"<ul> <li>Deck</li> <li>Code from  original repo</li> </ul>"},{"location":"knowledge-sharing/4%20-%20Monitoring%20and%20Observability%20in%20LLM%20Application/material/demo-langfuse/","title":"README","text":""},{"location":"knowledge-sharing/4%20-%20Monitoring%20and%20Observability%20in%20LLM%20Application/material/demo-langfuse/#check-each-folder-to-setup-open-in-github","title":"check each folder to setup (open in github)","text":"<ul> <li> <p><code>chat-api</code> - to setup chat api service</p> </li> <li> <p><code>chat-app</code> - to setup chat application</p> </li> <li> <p><code>infra</code> - to setup infra</p> </li> </ul>"},{"location":"knowledge-sharing/4%20-%20Monitoring%20and%20Observability%20in%20LLM%20Application/material/demo-langfuse/chat-api/","title":"Chat API","text":""},{"location":"knowledge-sharing/4%20-%20Monitoring%20and%20Observability%20in%20LLM%20Application/material/demo-langfuse/chat-api/#how-to-install-service","title":"How to install service","text":"<pre><code>poetry install\n</code></pre>"},{"location":"knowledge-sharing/4%20-%20Monitoring%20and%20Observability%20in%20LLM%20Application/material/demo-langfuse/chat-api/#shell-into-environment","title":"Shell into environment","text":"<pre><code>poetry shell\n</code></pre>"},{"location":"knowledge-sharing/4%20-%20Monitoring%20and%20Observability%20in%20LLM%20Application/material/demo-langfuse/chat-api/#how-to-start-service","title":"How to start service","text":"<pre><code>uvicorn main:app --host 0.0.0.0 --port 8080 --reload\n</code></pre>"},{"location":"knowledge-sharing/4%20-%20Monitoring%20and%20Observability%20in%20LLM%20Application/material/demo-langfuse/chat-app/","title":"README","text":"<p>This is a Next.js project bootstrapped with <code>create-next-app</code>.</p>"},{"location":"knowledge-sharing/4%20-%20Monitoring%20and%20Observability%20in%20LLM%20Application/material/demo-langfuse/chat-app/#getting-started","title":"Getting Started","text":"<p>First, run the development server:</p> <pre><code>npm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n</code></pre> <p>Open http://localhost:3000 with your browser to see the result.</p> <p>You can start editing the page by modifying <code>app/page.tsx</code>. The page auto-updates as you edit the file.</p> <p>This project uses <code>next/font</code> to automatically optimize and load Inter, a custom Google Font.</p>"},{"location":"knowledge-sharing/4%20-%20Monitoring%20and%20Observability%20in%20LLM%20Application/material/demo-langfuse/chat-app/#learn-more","title":"Learn More","text":"<p>To learn more about Next.js, take a look at the following resources:</p> <ul> <li>Next.js Documentation - learn about Next.js features and API.</li> <li>Learn Next.js - an interactive Next.js tutorial.</li> </ul> <p>You can check out the Next.js GitHub repository - your feedback and contributions are welcome!</p>"},{"location":"knowledge-sharing/4%20-%20Monitoring%20and%20Observability%20in%20LLM%20Application/material/demo-langfuse/chat-app/#deploy-on-vercel","title":"Deploy on Vercel","text":"<p>The easiest way to deploy your Next.js app is to use the Vercel Platform from the creators of Next.js.</p> <p>Check out our Next.js deployment documentation for more details.</p>"},{"location":"knowledge-sharing/4%20-%20Monitoring%20and%20Observability%20in%20LLM%20Application/material/demo-langfuse/infra/","title":"README","text":""},{"location":"knowledge-sharing/4%20-%20Monitoring%20and%20Observability%20in%20LLM%20Application/material/demo-langfuse/infra/#docker-compose","title":"Docker Compose","text":"<pre><code>docker compose up\n</code></pre>"},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/","title":"Optimizing LLM with LoRA","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/#resource","title":"Resource","text":"<p>Video | Material | Notebooks | Knowledge Summary</p>"},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/#summary","title":"Summary","text":"<p>Check out knowledge summary.</p>"},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/content/","title":"content","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/content/#files","title":"Files","text":"<ul> <li>Deck</li> <li>Code from  original repo</li> </ul>"},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/","title":"Fintune LLM with LoRa","text":"<p>I hope this project provides the solution you're looking for.</p>"},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/#notebooks","title":"Notebooks","text":"<p>1.Generate_json</p> <p>2.Json_to_text</p> <p>3.Finetuning</p> <p>4.Evaluate</p>"},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/01.Generate_json/","title":"1.Generate_json","text":"In\u00a0[\u00a0]: Copied! <pre># !pip install Faker\n</pre> # !pip install Faker In\u00a0[\u00a0]: Copied! <pre>from faker import Faker\nfrom faker.providers import DynamicProvider\nimport random\nimport pandas as pd\n\njob_provider = DynamicProvider(\n     provider_name=\"job\",\n     elements=[\n         \"student\", \"doctor\", \"nurse\", \"teacher\", \"software enginerr\",\n         \"data science\", \"data engineer\", \"tester\", \"data analyst\",\n         \"lawyer\", \"mechanic\", \"accountant\", \"sales\", \"chef\", \"police\",\n         \"architect\", \"graphic designer\", \"plumber\", \"marketing\", \"dentist\",\n         \"electrician\"\n        ],\n)\n\nfake = Faker()\n\nfake.add_provider(job_provider)\n</pre> from faker import Faker from faker.providers import DynamicProvider import random import pandas as pd  job_provider = DynamicProvider(      provider_name=\"job\",      elements=[          \"student\", \"doctor\", \"nurse\", \"teacher\", \"software enginerr\",          \"data science\", \"data engineer\", \"tester\", \"data analyst\",          \"lawyer\", \"mechanic\", \"accountant\", \"sales\", \"chef\", \"police\",          \"architect\", \"graphic designer\", \"plumber\", \"marketing\", \"dentist\",          \"electrician\"         ], )  fake = Faker()  fake.add_provider(job_provider) In\u00a0[\u00a0]: Copied! <pre>column = [\"uid\", \"name\", \"age\", \"job\"]\n</pre> column = [\"uid\", \"name\", \"age\", \"job\"] In\u00a0[\u00a0]: Copied! <pre>data = []\nfor i in range(2000):\n    data.append(\n        (\"id\"+str(i).rjust(5, \"0\"), fake.name(), random.randint(10, 70), fake.job())\n    )\n</pre> data = [] for i in range(2000):     data.append(         (\"id\"+str(i).rjust(5, \"0\"), fake.name(), random.randint(10, 70), fake.job())     ) In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(data, columns=column)\n</pre> df = pd.DataFrame(data, columns=column) In\u00a0[\u00a0]: Copied! <pre>df.to_json('./dataset/raw.json', index=False)\ndf.to_csv('./dataset/raw.csv', index=False)\n</pre> df.to_json('./dataset/raw.json', index=False) df.to_csv('./dataset/raw.csv', index=False) In\u00a0[\u00a0]: Copied! <pre>df\n</pre> df In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/02.Json_to_text/","title":"2.Json_to_text","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom openai import OpenAI\nimport re\nimport time\nimport pickle\n</pre> import pandas as pd from openai import OpenAI import re import time import pickle In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv('./dataset/raw.csv', )\n</pre> df = pd.read_csv('./dataset/raw.csv', ) In\u00a0[\u00a0]: Copied! <pre>df\n</pre> df In\u00a0[\u00a0]: Copied! <pre>client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n</pre> client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\") In\u00a0[\u00a0]: Copied! <pre>def get_text_between_quotes(input_string):\n    pattern = r'\"([^\"]*)\"'\n    matches = re.findall(pattern, input_string)\n    try:\n        return matches[0]\n    except Exception as e:\n        return input_string\n</pre> def get_text_between_quotes(input_string):     pattern = r'\"([^\"]*)\"'     matches = re.findall(pattern, input_string)     try:         return matches[0]     except Exception as e:         return input_string In\u00a0[\u00a0]: Copied! <pre>promt = \"\"\"\nGenerate formal script 1 paragraph to introduce with these information\n- name: {}\n- age: {}\n- job: {}\n\"\"\"\n\ndata = df.loc[0]\nstart_time = time.time()\ncompletion = client.chat.completions.create(\n  model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"Provide only answer don't repeat the question.\"},\n    {\"role\": \"user\", \"content\": promt.format(data[\"name\"], data[\"age\"], data[\"job\"])}\n  ],\n  temperature=0,\n)\ncontent = completion.choices[0].message.content\ntext = get_text_between_quotes(content)\nend_time = time.time()\nprint(f\"Time use: {end_time - start_time} -- text: {text}\")\n</pre> promt = \"\"\" Generate formal script 1 paragraph to introduce with these information - name: {} - age: {} - job: {} \"\"\"  data = df.loc[0] start_time = time.time() completion = client.chat.completions.create(   model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",   messages=[     {\"role\": \"system\", \"content\": \"Provide only answer don't repeat the question.\"},     {\"role\": \"user\", \"content\": promt.format(data[\"name\"], data[\"age\"], data[\"job\"])}   ],   temperature=0, ) content = completion.choices[0].message.content text = get_text_between_quotes(content) end_time = time.time() print(f\"Time use: {end_time - start_time} -- text: {text}\") In\u00a0[\u00a0]: Copied! <pre>promt = \"\"\"\nGenerate informal script 1 paragraph to introduce with these information\n- name: {}\n- age: {}\n- job: {}\n\"\"\"\n\ndata = df.loc[0]\nstart_time = time.time()\ncompletion = client.chat.completions.create(\n  model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"Provide only answer don't repeat the question.\"},\n    {\"role\": \"user\", \"content\": promt.format(data[\"name\"], data[\"age\"], data[\"job\"])}\n  ],\n  temperature=0,\n)\ncontent = completion.choices[0].message.content\ntext = get_text_between_quotes(content)\nend_time = time.time()\nprint(f\"Time use: {end_time - start_time} -- text: {text}\")\n</pre> promt = \"\"\" Generate informal script 1 paragraph to introduce with these information - name: {} - age: {} - job: {} \"\"\"  data = df.loc[0] start_time = time.time() completion = client.chat.completions.create(   model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",   messages=[     {\"role\": \"system\", \"content\": \"Provide only answer don't repeat the question.\"},     {\"role\": \"user\", \"content\": promt.format(data[\"name\"], data[\"age\"], data[\"job\"])}   ],   temperature=0, ) content = completion.choices[0].message.content text = get_text_between_quotes(content) end_time = time.time() print(f\"Time use: {end_time - start_time} -- text: {text}\") In\u00a0[\u00a0]: Copied! <pre>promt = \"\"\"\nGenerate 1 paragraph story which include following information \n- name: {}\n- age: {}\n- job: {}\n\"\"\"\n\ndata = df.loc[0]\nstart_time = time.time()\ncompletion = client.chat.completions.create(\n  model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"Provide only answer don't repeat the question.\"},\n    {\"role\": \"user\", \"content\": promt.format(data[\"name\"], data[\"age\"], data[\"job\"])}\n  ],\n  temperature=0,\n)\ncontent = completion.choices[0].message.content\ntext = get_text_between_quotes(content)\nend_time = time.time()\nprint(f\"Time use: {end_time - start_time} -- text: {text}\")\n</pre> promt = \"\"\" Generate 1 paragraph story which include following information  - name: {} - age: {} - job: {} \"\"\"  data = df.loc[0] start_time = time.time() completion = client.chat.completions.create(   model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",   messages=[     {\"role\": \"system\", \"content\": \"Provide only answer don't repeat the question.\"},     {\"role\": \"user\", \"content\": promt.format(data[\"name\"], data[\"age\"], data[\"job\"])}   ],   temperature=0, ) content = completion.choices[0].message.content text = get_text_between_quotes(content) end_time = time.time() print(f\"Time use: {end_time - start_time} -- text: {text}\") In\u00a0[\u00a0]: Copied! <pre>promt = \"\"\"\nGenerate 1 paragraph story which include following information \n- name: {}\n- age: {}\n- job: {}\n\"\"\"\n\ndata = df.loc[0]\nstart_time = time.time()\ncompletion = client.chat.completions.create(\n  model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"Provide only answer don't repeat the question.\"},\n    {\"role\": \"user\", \"content\": promt.format(data[\"name\"], data[\"age\"], data[\"job\"])}\n  ],\n  temperature=0,\n)\ncontent = completion.choices[0].message.content\ntext = get_text_between_quotes(content)\nend_time = time.time()\nprint(f\"Time use: {end_time - start_time} -- text: {text}\")\n</pre> promt = \"\"\" Generate 1 paragraph story which include following information  - name: {} - age: {} - job: {} \"\"\"  data = df.loc[0] start_time = time.time() completion = client.chat.completions.create(   model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",   messages=[     {\"role\": \"system\", \"content\": \"Provide only answer don't repeat the question.\"},     {\"role\": \"user\", \"content\": promt.format(data[\"name\"], data[\"age\"], data[\"job\"])}   ],   temperature=0, ) content = completion.choices[0].message.content text = get_text_between_quotes(content) end_time = time.time() print(f\"Time use: {end_time - start_time} -- text: {text}\") In\u00a0[\u00a0]: Copied! <pre>formal_script = {}\n# for temp in [0, 0.3, 0.8, 1.3]:\nfor temp in [0.8]:\n    promt = \"\"\"\n    Generate formal script 1 paragraph to introduce with these information\n    - name: {}\n    - age: {}\n    - job: {}\n    \"\"\"\n\n    key = f\"temp_{temp}\"\n    formal_script[key] = {}\n    for _, data in df.iterrows():\n        completion = client.chat.completions.create(\n          model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n          messages=[\n            {\"role\": \"system\", \"content\": \"Provide only answer don't repeat the question.\"},\n            {\"role\": \"user\", \"content\": promt.format(data[\"name\"], data[\"age\"], data[\"job\"])}\n          ],\n          temperature=temp,\n        )\n        content = completion.choices[0].message.content\n        try:\n            text = get_text_between_quotes(content)\n        except Exception as e:\n            text = content\n        formal_script[key][data[\"uid\"]] = text\n</pre> formal_script = {} # for temp in [0, 0.3, 0.8, 1.3]: for temp in [0.8]:     promt = \"\"\"     Generate formal script 1 paragraph to introduce with these information     - name: {}     - age: {}     - job: {}     \"\"\"      key = f\"temp_{temp}\"     formal_script[key] = {}     for _, data in df.iterrows():         completion = client.chat.completions.create(           model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",           messages=[             {\"role\": \"system\", \"content\": \"Provide only answer don't repeat the question.\"},             {\"role\": \"user\", \"content\": promt.format(data[\"name\"], data[\"age\"], data[\"job\"])}           ],           temperature=temp,         )         content = completion.choices[0].message.content         try:             text = get_text_between_quotes(content)         except Exception as e:             text = content         formal_script[key][data[\"uid\"]] = text In\u00a0[\u00a0]: Copied! <pre>with open('./dataset/formal_script.pkl', 'wb') as fp:\n    pickle.dump(formal_script, fp)\n</pre> with open('./dataset/formal_script.pkl', 'wb') as fp:     pickle.dump(formal_script, fp) In\u00a0[\u00a0]: Copied! <pre>informal_script = {}\n# for temp in [0, 0.3, 0.8, 1.3]:\nfor temp in [0.8]:\n    promt = \"\"\"\n    Generate informal script 1 paragraph to introduce with these information\n    - name: {}\n    - age: {}\n    - job: {}\n    \"\"\"\n\n    key = f\"temp_{temp}\"\n    informal_script[key] = {}\n    for _, data in df.iterrows():\n        completion = client.chat.completions.create(\n          model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n          messages=[\n            {\"role\": \"system\", \"content\": \"Provide only answer don't repeat the question.\"},\n            {\"role\": \"user\", \"content\": promt.format(data[\"name\"], data[\"age\"], data[\"job\"])}\n          ],\n          temperature=temp,\n        )\n        content = completion.choices[0].message.content\n        try:\n            text = get_text_between_quotes(content)\n        except Exception as e:\n            text = content\n        informal_script[key][data[\"uid\"]] = text\n</pre> informal_script = {} # for temp in [0, 0.3, 0.8, 1.3]: for temp in [0.8]:     promt = \"\"\"     Generate informal script 1 paragraph to introduce with these information     - name: {}     - age: {}     - job: {}     \"\"\"      key = f\"temp_{temp}\"     informal_script[key] = {}     for _, data in df.iterrows():         completion = client.chat.completions.create(           model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",           messages=[             {\"role\": \"system\", \"content\": \"Provide only answer don't repeat the question.\"},             {\"role\": \"user\", \"content\": promt.format(data[\"name\"], data[\"age\"], data[\"job\"])}           ],           temperature=temp,         )         content = completion.choices[0].message.content         try:             text = get_text_between_quotes(content)         except Exception as e:             text = content         informal_script[key][data[\"uid\"]] = text In\u00a0[\u00a0]: Copied! <pre>with open('./dataset/informal_script.pkl', 'wb') as fp:\n    pickle.dump(informal_script, fp)\n</pre> with open('./dataset/informal_script.pkl', 'wb') as fp:     pickle.dump(informal_script, fp) In\u00a0[\u00a0]: Copied! <pre>novel_script = {}\n# for temp in [0, 0.3, 0.8, 1.3]:\nfor temp in [0.8]:\n    promt = \"\"\"\n    Generate 1 paragraph story which include following information \n    - name: {}\n    - age: {}\n    - job: {}\n    \"\"\"\n\n    key = f\"temp_{temp}\"\n    novel_script[key] = {}\n    for _, data in df.iterrows():\n        completion = client.chat.completions.create(\n          model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n          messages=[\n            {\"role\": \"system\", \"content\": \"Provide only answer don't repeat the question.\"},\n            {\"role\": \"user\", \"content\": promt.format(data[\"name\"], data[\"age\"], data[\"job\"])}\n          ],\n          temperature=temp,\n        )\n        content = completion.choices[0].message.content\n        try:\n            text = get_text_between_quotes(content)\n        except Exception as e:\n            text = content\n        novel_script[key][data[\"uid\"]] = text\n</pre> novel_script = {} # for temp in [0, 0.3, 0.8, 1.3]: for temp in [0.8]:     promt = \"\"\"     Generate 1 paragraph story which include following information      - name: {}     - age: {}     - job: {}     \"\"\"      key = f\"temp_{temp}\"     novel_script[key] = {}     for _, data in df.iterrows():         completion = client.chat.completions.create(           model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",           messages=[             {\"role\": \"system\", \"content\": \"Provide only answer don't repeat the question.\"},             {\"role\": \"user\", \"content\": promt.format(data[\"name\"], data[\"age\"], data[\"job\"])}           ],           temperature=temp,         )         content = completion.choices[0].message.content         try:             text = get_text_between_quotes(content)         except Exception as e:             text = content         novel_script[key][data[\"uid\"]] = text In\u00a0[\u00a0]: Copied! <pre>with open('./dataset/novel_script.pkl', 'wb') as fp:\n    pickle.dump(novel_script, fp)\n</pre> with open('./dataset/novel_script.pkl', 'wb') as fp:     pickle.dump(novel_script, fp) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/02.Json_to_text/#zero-shot-generate","title":"Zero shot generate\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/02.Json_to_text/#test","title":"Test\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/02.Json_to_text/#generate-zero-shot","title":"Generate zero shot\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/03.Finetuning/","title":"3.Finetuning","text":"In\u00a0[\u00a0]: Copied! <pre># !pip install packaging\n# !pip install ninja\n# !pip install flash-attn --no-build-isolation\n# https://github.com/bdashore3/flash-attention/releases\n# !pip install peft transformers datasets\n# https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/5-Fine%20Tuning/LoRA_Tuning_PEFT.ipynb\n</pre> # !pip install packaging # !pip install ninja # !pip install flash-attn --no-build-isolation # https://github.com/bdashore3/flash-attention/releases # !pip install peft transformers datasets # https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/5-Fine%20Tuning/LoRA_Tuning_PEFT.ipynb In\u00a0[\u00a0]: Copied! <pre># Python      3.11.7\n# GPU         4070TI 12GB\n# Cuda        cuda_12.1.r12.1\n# Library\n# torch       2.2.2+cu121\n# flash_attn  2.5.9.post1\n</pre> # Python      3.11.7 # GPU         4070TI 12GB # Cuda        cuda_12.1.r12.1 # Library # torch       2.2.2+cu121 # flash_attn  2.5.9.post1 In\u00a0[\u00a0]: Copied! <pre>from peft import (\n    get_peft_model, \n    LoraConfig, \n    TaskType, \n    prepare_model_for_kbit_training\n)\nimport transformers\nimport torch\nimport pickle\nimport time\nimport pandas as pd\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n)\nfrom datasets.dataset_dict import DatasetDict\nfrom datasets import Dataset\nimport os\n\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n</pre> from peft import (     get_peft_model,      LoraConfig,      TaskType,      prepare_model_for_kbit_training ) import transformers import torch import pickle import time import pandas as pd from transformers import (     AutoModelForCausalLM,     AutoTokenizer,     BitsAndBytesConfig,     TrainingArguments,     Trainer, ) from datasets.dataset_dict import DatasetDict from datasets import Dataset import os  os.environ['CUDA_LAUNCH_BLOCKING'] = '1' In\u00a0[\u00a0]: Copied! <pre>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n</pre> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(f\"Using device: {device}\") In\u00a0[\u00a0]: Copied! <pre>compute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_8bit=True,\n        llm_int8_threshold=6.0,\n        # llm_int8_skip_modules=None,\n        # llm_int8_enable_fp32_cpu_offload=False,\n        # llm_int8_has_fp16_weight=False,\n    \n        # load_in_4bit=True,\n        # bnb_4bit_quant_type='nf4',\n        # bnb_4bit_compute_dtype=compute_dtype,\n        # bnb_4bit_use_double_quant=False,\n    )\n</pre> compute_dtype = getattr(torch, \"float16\") bnb_config = BitsAndBytesConfig(         load_in_8bit=True,         llm_int8_threshold=6.0,         # llm_int8_skip_modules=None,         # llm_int8_enable_fp32_cpu_offload=False,         # llm_int8_has_fp16_weight=False,              # load_in_4bit=True,         # bnb_4bit_quant_type='nf4',         # bnb_4bit_compute_dtype=compute_dtype,         # bnb_4bit_use_double_quant=False,     ) In\u00a0[\u00a0]: Copied! <pre>model_name_or_path = \"microsoft/Phi-3-mini-4k-instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path, \n    quantization_config=bnb_config,\n    device_map=\"auto\", \n    trust_remote_code=True,\n    attn_implementation=\"flash_attention_2\",\n    use_cache=False\n) # load the model\n</pre> model_name_or_path = \"microsoft/Phi-3-mini-4k-instruct\" model = AutoModelForCausalLM.from_pretrained(     model_name_or_path,      quantization_config=bnb_config,     device_map=\"auto\",      trust_remote_code=True,     attn_implementation=\"flash_attention_2\",     use_cache=False ) # load the model In\u00a0[\u00a0]: Copied! <pre>model = prepare_model_for_kbit_training(model)\nmodel.gradient_checkpointing_enable()\n</pre> model = prepare_model_for_kbit_training(model) model.gradient_checkpointing_enable() In\u00a0[\u00a0]: Copied! <pre>model\n</pre> model In\u00a0[\u00a0]: Copied! <pre>peft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM, \n    inference_mode=False, \n    r=32, \n    lora_alpha=16, \n    lora_dropout=0.1,\n    # target_modules='all-linear'\n    target_modules=[\"qkv_proj\"] # optional, you can target specific layers using this\n    # target_modules=[\"v_proj\", \"q_proj\"]\n) # create LoRA config for the finetuning\n\npeft_model = get_peft_model(model, peft_config) # create a model ready for LoRA finetuning\n</pre> peft_config = LoraConfig(     task_type=TaskType.CAUSAL_LM,      inference_mode=False,      r=32,      lora_alpha=16,      lora_dropout=0.1,     # target_modules='all-linear'     target_modules=[\"qkv_proj\"] # optional, you can target specific layers using this     # target_modules=[\"v_proj\", \"q_proj\"] ) # create LoRA config for the finetuning  peft_model = get_peft_model(model, peft_config) # create a model ready for LoRA finetuning In\u00a0[\u00a0]: Copied! <pre>peft_model\n</pre> peft_model In\u00a0[\u00a0]: Copied! <pre>peft_model.print_trainable_parameters()\n</pre> peft_model.print_trainable_parameters()  In\u00a0[\u00a0]: Copied! <pre>with open('./dataset/full_formal_script_temp0.8.pkl', 'rb') as fp:\n    data_formal = pickle.load(fp)\n\nwith open('./dataset/full_informal_script_temp0.8.pkl', 'rb') as fp:\n    data_informal = pickle.load(fp)\n\nwith open('./dataset/full_novel_script_temp0.8.pkl', 'rb') as fp:\n    data_novel = pickle.load(fp)\n\ndf = pd.read_csv('./dataset/raw.csv', index_col='uid')\n# https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2\n</pre> with open('./dataset/full_formal_script_temp0.8.pkl', 'rb') as fp:     data_formal = pickle.load(fp)  with open('./dataset/full_informal_script_temp0.8.pkl', 'rb') as fp:     data_informal = pickle.load(fp)  with open('./dataset/full_novel_script_temp0.8.pkl', 'rb') as fp:     data_novel = pickle.load(fp)  df = pd.read_csv('./dataset/raw.csv', index_col='uid') # https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2 In\u00a0[\u00a0]: Copied! <pre>df['json'] = df.apply(lambda x : {'name': x['name'], 'age': x['age'], 'job': x['job']}, axis=1)\ndf\n</pre> df['json'] = df.apply(lambda x : {'name': x['name'], 'age': x['age'], 'job': x['job']}, axis=1) df In\u00a0[\u00a0]: Copied! <pre>for temp in data_formal:\n    for uid in data_formal[temp]:\n        df.loc[uid, 'formal'] = data_formal[temp][uid]\n\nfor temp in data_informal:\n    for uid in data_informal[temp]:\n        df.loc[uid, 'informal'] = data_informal[temp][uid]\n        \nfor temp in data_novel:\n    for uid in data_novel[temp]:\n        df.loc[uid, 'novel'] = data_novel[temp][uid]\n</pre> for temp in data_formal:     for uid in data_formal[temp]:         df.loc[uid, 'formal'] = data_formal[temp][uid]  for temp in data_informal:     for uid in data_informal[temp]:         df.loc[uid, 'informal'] = data_informal[temp][uid]          for temp in data_novel:     for uid in data_novel[temp]:         df.loc[uid, 'novel'] = data_novel[temp][uid] In\u00a0[\u00a0]: Copied! <pre>train_ratio = 0.9\nindex = int(len(df)*train_ratio)\ntrain_df, test_df = df[:index], df[index:]\n</pre> train_ratio = 0.9 index = int(len(df)*train_ratio) train_df, test_df = df[:index], df[index:] In\u00a0[\u00a0]: Copied! <pre>train_prep_df = train_df.reset_index()[['uid', 'json', 'formal', 'informal', 'novel']].melt(\n    id_vars=['uid','json'],\n    var_name=\"type\",\n    value_name=\"context\"\n).sort_values('uid')\ntrain_prep_df = train_prep_df[['json', 'context']]\n\ntest_prep_df = test_df.reset_index()[['uid', 'json', 'formal', 'informal', 'novel']].melt(\n    id_vars=['uid','json'],\n    var_name=\"type\",\n    value_name=\"context\"\n).sort_values('uid')\ntest_prep_df = test_prep_df[['json', 'context']]\n</pre> train_prep_df = train_df.reset_index()[['uid', 'json', 'formal', 'informal', 'novel']].melt(     id_vars=['uid','json'],     var_name=\"type\",     value_name=\"context\" ).sort_values('uid') train_prep_df = train_prep_df[['json', 'context']]  test_prep_df = test_df.reset_index()[['uid', 'json', 'formal', 'informal', 'novel']].melt(     id_vars=['uid','json'],     var_name=\"type\",     value_name=\"context\" ).sort_values('uid') test_prep_df = test_prep_df[['json', 'context']] In\u00a0[\u00a0]: Copied! <pre># https://stackoverflow.com/questions/67852880/how-can-i-handle-this-datasets-to-create-a-datasetdict\ndataset = DatasetDict({\n    'train': Dataset.from_pandas(train_prep_df, preserve_index=False),\n    'test': Dataset.from_pandas(test_prep_df, preserve_index=False)\n})\n</pre> # https://stackoverflow.com/questions/67852880/how-can-i-handle-this-datasets-to-create-a-datasetdict dataset = DatasetDict({     'train': Dataset.from_pandas(train_prep_df, preserve_index=False),     'test': Dataset.from_pandas(test_prep_df, preserve_index=False) }) In\u00a0[\u00a0]: Copied! <pre>dataset\n</pre> dataset In\u00a0[\u00a0]: Copied! <pre>token_fn = AutoTokenizer.from_pretrained(model_name_or_path)\n</pre> token_fn = AutoTokenizer.from_pretrained(model_name_or_path) In\u00a0[\u00a0]: Copied! <pre>def create_prompt_formats(sample, add_result=False):\n    ################# Version = 1 ################\n#     prompt = f\"\"\"You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.\n# Extract name, age, job from the sentence to json format. if the information doesn't exsits fill null.\n# Sentence: '{sample['context']}'\n# \"\"\"\n#     if add_result:\n#         prompt += f\"{sample['json']}\"\n\n    ################# Version = 2 ################\n    \n    prompt = f\"\"\"&lt;|system|&gt;\nYou are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.&lt;|end|&gt;\n&lt;|user|&gt;\nExtract name, age, job from the sentence to json format. if the information doesn't exsits fill null.\n\nSentence: '{sample['context']}'&lt;|end|&gt;\n&lt;|assistant|&gt;\n\"\"\"\n    prompt += f\"{sample['json']}&lt;|end|&gt;\"\n    sample[\"text\"] = prompt\n    # sample[\"json\"] = str(sample['json'])\n    \n    return sample\n</pre> def create_prompt_formats(sample, add_result=False):     ################# Version = 1 ################ #     prompt = f\"\"\"You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability. # Extract name, age, job from the sentence to json format. if the information doesn't exsits fill null. # Sentence: '{sample['context']}' # \"\"\" #     if add_result: #         prompt += f\"{sample['json']}\"      ################# Version = 2 ################          prompt = f\"\"\"&lt;|system|&gt; You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.&lt;|end|&gt; &lt;|user|&gt; Extract name, age, job from the sentence to json format. if the information doesn't exsits fill null.  Sentence: '{sample['context']}'&lt;|end|&gt; &lt;|assistant|&gt; \"\"\"     prompt += f\"{sample['json']}&lt;|end|&gt;\"     sample[\"text\"] = prompt     # sample[\"json\"] = str(sample['json'])          return sample In\u00a0[\u00a0]: Copied! <pre>train_dataset = dataset['train'].map(create_prompt_formats, fn_kwargs={'add_result': True}, remove_columns=['json', 'context'])\neval_dataset = dataset['test'].map(create_prompt_formats, fn_kwargs={'add_result': False}, remove_columns=['json', 'context'])\n</pre> train_dataset = dataset['train'].map(create_prompt_formats, fn_kwargs={'add_result': True}, remove_columns=['json', 'context']) eval_dataset = dataset['test'].map(create_prompt_formats, fn_kwargs={'add_result': False}, remove_columns=['json', 'context']) In\u00a0[\u00a0]: Copied! <pre>train_dataset\n</pre> train_dataset In\u00a0[\u00a0]: Copied! <pre>def tokenize_function(examples):\n    return token_fn(\n        examples[\"text\"], \n        truncation=True, \n        padding=\"max_length\", \n        max_length=512\n    )\n\ntokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\ntokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n</pre> def tokenize_function(examples):     return token_fn(         examples[\"text\"],          truncation=True,          padding=\"max_length\",          max_length=512     )  tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True) tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True) In\u00a0[\u00a0]: Copied! <pre>tokenized_train_dataset\n</pre> tokenized_train_dataset In\u00a0[\u00a0]: Copied! <pre>import inspect\nmodel_to_inspect = peft_model.get_base_model()\nsignature = inspect.signature(model_to_inspect.forward)\nlist(signature.parameters.keys())\n</pre> import inspect model_to_inspect = peft_model.get_base_model() signature = inspect.signature(model_to_inspect.forward) list(signature.parameters.keys()) In\u00a0[\u00a0]: Copied! <pre>output_dir = f'./Phi-3-mini-4k-instruct-8Blora-text2json-training-clean-{str(int(time.time()))}'\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n\n    gradient_accumulation_steps=4,\n    save_steps=10,\n    save_total_limit=50,\n    logging_steps=1,\n    \n    learning_rate=1e-3,\n    weight_decay=0.01,\n    remove_unused_columns=True,\n    fp16=True\n)\n</pre> output_dir = f'./Phi-3-mini-4k-instruct-8Blora-text2json-training-clean-{str(int(time.time()))}'  training_args = TrainingArguments(     output_dir=output_dir,     num_train_epochs=3,     per_device_train_batch_size=4,     per_device_eval_batch_size=4,      gradient_accumulation_steps=4,     save_steps=10,     save_total_limit=50,     logging_steps=1,          learning_rate=1e-3,     weight_decay=0.01,     remove_unused_columns=True,     fp16=True ) In\u00a0[\u00a0]: Copied! <pre>trainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_eval_dataset,\n    data_collator=transformers.DataCollatorForLanguageModeling(token_fn, mlm=False)\n)\n</pre> trainer = Trainer(     model=peft_model,     args=training_args,     train_dataset=tokenized_train_dataset,     eval_dataset=tokenized_eval_dataset,     data_collator=transformers.DataCollatorForLanguageModeling(token_fn, mlm=False) ) In\u00a0[\u00a0]: Copied! <pre>try:\n    trainer.train()\nexcept RuntimeError as e:\n    print(f\"Error during training: {e}\")\n    print(\"Attempting to continue training on CPU...\")\n    device = torch.device(\"cpu\")\n    model = model.to(device)\n    training_args.fp16 = False\n    training_args.per_device_train_batch_size = 1\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n        data_collator=data_collator,\n    )\n    trainer.train()\n</pre> try:     trainer.train() except RuntimeError as e:     print(f\"Error during training: {e}\")     print(\"Attempting to continue training on CPU...\")     device = torch.device(\"cpu\")     model = model.to(device)     training_args.fp16 = False     training_args.per_device_train_batch_size = 1     trainer = Trainer(         model=model,         args=training_args,         train_dataset=tokenized_dataset,         data_collator=data_collator,     )     trainer.train() In\u00a0[\u00a0]: Copied! <pre>peft_model.save_pretrained('./Tuning/checkpoint/Phi-3-mini-4k-instruct')\n</pre> peft_model.save_pretrained('./Tuning/checkpoint/Phi-3-mini-4k-instruct') In\u00a0[\u00a0]: Copied! <pre>token_fn.save_pretrained('./Tuning/checkpoint/Phi-3-mini-4k-instruct/tokenize')\n</pre> token_fn.save_pretrained('./Tuning/checkpoint/Phi-3-mini-4k-instruct/tokenize') In\u00a0[\u00a0]: Copied! <pre>def create_prompt_formats_eval(sample):\n    prompt = f\"\"\"&lt;|system|&gt;\nYou are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.&lt;|end|&gt;\n&lt;|user|&gt;\nExtract name, age, job from the sentence to json format. if the information doesn't exsits fill null.\n\nSentence: '{sample['context']}'&lt;|end|&gt;\n&lt;|assistant|&gt;\n\"\"\"\n    sample[\"text\"] = prompt\n    \n    return sample\n</pre> def create_prompt_formats_eval(sample):     prompt = f\"\"\"&lt;|system|&gt; You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.&lt;|end|&gt; &lt;|user|&gt; Extract name, age, job from the sentence to json format. if the information doesn't exsits fill null.  Sentence: '{sample['context']}'&lt;|end|&gt; &lt;|assistant|&gt; \"\"\"     sample[\"text\"] = prompt          return sample In\u00a0[\u00a0]: Copied! <pre>index = 1\n\ninputs = token_fn(\n    eval[index]['text'], \n    truncation=True, \n    padding=\"max_length\", \n    max_length=512,\n    return_tensors=\"pt\"\n).to(peft_model.device)\n        \n# Generate the prediction\noutputs = peft_model.generate(**inputs, max_new_tokens=512)\n\n# # Decode the output\npredicted_text = token_fn.decode(outputs[0], skip_special_tokens=True)\n\nresult = re.findall('{.*}', predicted_text)[0]\n</pre> index = 1  inputs = token_fn(     eval[index]['text'],      truncation=True,      padding=\"max_length\",      max_length=512,     return_tensors=\"pt\" ).to(peft_model.device)          # Generate the prediction outputs = peft_model.generate(**inputs, max_new_tokens=512)  # # Decode the output predicted_text = token_fn.decode(outputs[0], skip_special_tokens=True)  result = re.findall('{.*}', predicted_text)[0]"},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/03.Finetuning/#load-model","title":"Load Model\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/03.Finetuning/#fine-tuning","title":"Fine-Tuning\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/03.Finetuning/#load-and-prepare-data","title":"Load and Prepare data\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/03.Finetuning/#test-the-model-with-zero-shot-inferencing","title":"Test the Model with Zero Shot Inferencing\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/03.Finetuning/#training-model","title":"Training model\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/03.Finetuning/#save-model","title":"Save model\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/03.Finetuning/#test-call-finetuned-model","title":"Test Call finetuned model\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/04.Evaluate/","title":"4.Evaluate","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom openai import OpenAI\nimport re\nimport time\nimport pickle\nimport re\n</pre> import pandas as pd from openai import OpenAI import re import time import pickle import re In\u00a0[\u00a0]: Copied! <pre>client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n</pre> client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\") In\u00a0[\u00a0]: Copied! <pre>def get_text_between_quotes(input_string):\n    pattern = r'\"([^\"]*)\"'\n    matches = re.findall(pattern, input_string)\n    try:\n        return matches[0]\n    except Exception as e:\n        return input_string\n</pre> def get_text_between_quotes(input_string):     pattern = r'\"([^\"]*)\"'     matches = re.findall(pattern, input_string)     try:         return matches[0]     except Exception as e:         return input_string In\u00a0[\u00a0]: Copied! <pre>with open('./dataset/full_formal_script_temp0.8.pkl', 'rb') as fp:\n    data_formal = pickle.load(fp)\n\nwith open('./dataset/full_informal_script_temp0.8.pkl', 'rb') as fp:\n    data_informal = pickle.load(fp)\n\nwith open('./dataset/full_novel_script_temp0.8.pkl', 'rb') as fp:\n    data_novel = pickle.load(fp)\n\ndf = pd.read_csv('./dataset/raw.csv', index_col='uid')\n</pre> with open('./dataset/full_formal_script_temp0.8.pkl', 'rb') as fp:     data_formal = pickle.load(fp)  with open('./dataset/full_informal_script_temp0.8.pkl', 'rb') as fp:     data_informal = pickle.load(fp)  with open('./dataset/full_novel_script_temp0.8.pkl', 'rb') as fp:     data_novel = pickle.load(fp)  df = pd.read_csv('./dataset/raw.csv', index_col='uid') In\u00a0[\u00a0]: Copied! <pre>data_novel\n</pre> data_novel In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel, PeftConfig\n</pre> import torch from transformers import AutoTokenizer, AutoModelForCausalLM from peft import PeftModel, PeftConfig In\u00a0[\u00a0]: Copied! <pre>model_path = \"./Tuning/checkpoint/Phi-3-mini-4k-instruct\"\ntoken_path = f\"{model_path}/tokenize\"\n</pre> model_path = \"./Tuning/checkpoint/Phi-3-mini-4k-instruct\" token_path = f\"{model_path}/tokenize\" In\u00a0[\u00a0]: Copied! <pre>tokenizer = AutoTokenizer.from_pretrained(token_path)\n</pre> tokenizer = AutoTokenizer.from_pretrained(token_path) In\u00a0[\u00a0]: Copied! <pre>peft_config = PeftConfig.from_pretrained(model_path)\n</pre> peft_config = PeftConfig.from_pretrained(model_path) In\u00a0[\u00a0]: Copied! <pre>base_model = AutoModelForCausalLM.from_pretrained(\n    peft_config.base_model_name_or_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n</pre> base_model = AutoModelForCausalLM.from_pretrained(     peft_config.base_model_name_or_path,     torch_dtype=torch.float16,     device_map=\"auto\", ) In\u00a0[\u00a0]: Copied! <pre>model = PeftModel.from_pretrained(base_model, model_path)\n</pre> model = PeftModel.from_pretrained(base_model, model_path) In\u00a0[\u00a0]: Copied! <pre>model = model.merge_and_unload()\n</pre> model = model.merge_and_unload() In\u00a0[\u00a0]: Copied! <pre>model.eval()\n</pre> model.eval() In\u00a0[\u00a0]: Copied! <pre>def generate_text(prompt, max_length=400, temperature=0.7):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_length=max_length,\n            num_return_sequences=1,\n            temperature=temperature,\n            do_sample=True,\n        )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n</pre> def generate_text(prompt, max_length=400, temperature=0.7):     inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)     with torch.no_grad():         outputs = model.generate(             **inputs,             max_length=max_length,             num_return_sequences=1,             temperature=temperature,             do_sample=True,         )     return tokenizer.decode(outputs[0], skip_special_tokens=True) In\u00a0[\u00a0]: Copied! <pre>def get_dict(text):\n    res = None\n    try:\n        res = re.findall('{.*}', text)[0]\n        res = eval(res)\n    except:\n        print(f\"Error: {text}\")\n    return res\n</pre> def get_dict(text):     res = None     try:         res = re.findall('{.*}', text)[0]         res = eval(res)     except:         print(f\"Error: {text}\")     return res In\u00a0[\u00a0]: Copied! <pre>prompt = \"\"\"&lt;|system|&gt;\nYou are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.&lt;|end|&gt;\n&lt;|user|&gt;\nExtract name, age, job from the sentence to json format. if the information doesn't exist fill null.\n\nSentence: 'John Doe is a 35-year-old software engineer with a passion for artificial intelligence.'&lt;|end|&gt;\n&lt;|assistant|&gt;\n\"\"\"\n\ngenerated_text_low_temp = generate_text(prompt, temperature=0.1)\nget_dict(generated_text_low_temp)\n</pre> prompt = \"\"\"&lt;|system|&gt; You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.&lt;|end|&gt; &lt;|user|&gt; Extract name, age, job from the sentence to json format. if the information doesn't exist fill null.  Sentence: 'John Doe is a 35-year-old software engineer with a passion for artificial intelligence.'&lt;|end|&gt; &lt;|assistant|&gt; \"\"\"  generated_text_low_temp = generate_text(prompt, temperature=0.1) get_dict(generated_text_low_temp) In\u00a0[\u00a0]: Copied! <pre>def call_llm(data):\n    promtp = \"\"\"Extract name, age, job from the sentence to json format. \n    if the information doesn't exsits fill null.\n    \n    sentence: '{}'\n    \"\"\"\n    \n    start_time = time.time()\n    completion = client.chat.completions.create(\n      model=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n      messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.\"},\n        {\"role\": \"user\", \"content\": promtp.format(data)}\n      ],\n      temperature=0,\n    )\n    end_time = time.time()\n    content = completion.choices[0].message.content\n    return end_time-start_time, content.strip().replace('null', '\\\"\\\"')\n\ndef call_llm_model(data):\n    prompt = f\"\"\"&lt;|system|&gt;\nYou are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.&lt;|end|&gt;\n&lt;|user|&gt;\nExtract name, age, job from the sentence to json format. if the information doesn't exist fill null.\n\nSentence: '{data}'&lt;|end|&gt;\n&lt;|assistant|&gt;\"\"\"\n    start_time = time.time()\n    generated_text_low_temp = generate_text(prompt, temperature=0.1)\n    end_time = time.time()\n    return end_time-start_time, get_dict(generated_text_low_temp)\n</pre> def call_llm(data):     promtp = \"\"\"Extract name, age, job from the sentence to json format.      if the information doesn't exsits fill null.          sentence: '{}'     \"\"\"          start_time = time.time()     completion = client.chat.completions.create(       model=\"microsoft/Phi-3-mini-4k-instruct-gguf\",       messages=[         {\"role\": \"system\", \"content\": \"You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.\"},         {\"role\": \"user\", \"content\": promtp.format(data)}       ],       temperature=0,     )     end_time = time.time()     content = completion.choices[0].message.content     return end_time-start_time, content.strip().replace('null', '\\\"\\\"')  def call_llm_model(data):     prompt = f\"\"\"&lt;|system|&gt; You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.&lt;|end|&gt; &lt;|user|&gt; Extract name, age, job from the sentence to json format. if the information doesn't exist fill null.  Sentence: '{data}'&lt;|end|&gt; &lt;|assistant|&gt;\"\"\"     start_time = time.time()     generated_text_low_temp = generate_text(prompt, temperature=0.1)     end_time = time.time()     return end_time-start_time, get_dict(generated_text_low_temp) In\u00a0[\u00a0]: Copied! <pre># Predict formal data\npredict_data = {}\nfor temp in data_formal:\n    for id in data_formal[temp]:\n        # _, result = call_llm(data_formal[temp][id]) # Before finetune\n        _, result = call_llm_model(data_formal[temp][id]) # After finetune\n        try:\n            if not isinstance(result, dict):\n                predict_data[id] = eval(result)\n            else:\n                predict_data[id] = result\n        except:\n            print(result)\n\n# with open('./dataset/predict_formal_script.pkl', 'wb') as fp:\n#     pickle.dump(predict_data, fp)\nwith open('./dataset/finetune_predict_formal_script.pkl', 'wb') as fp:\n    pickle.dump(predict_data, fp)\n</pre> # Predict formal data predict_data = {} for temp in data_formal:     for id in data_formal[temp]:         # _, result = call_llm(data_formal[temp][id]) # Before finetune         _, result = call_llm_model(data_formal[temp][id]) # After finetune         try:             if not isinstance(result, dict):                 predict_data[id] = eval(result)             else:                 predict_data[id] = result         except:             print(result)  # with open('./dataset/predict_formal_script.pkl', 'wb') as fp: #     pickle.dump(predict_data, fp) with open('./dataset/finetune_predict_formal_script.pkl', 'wb') as fp:     pickle.dump(predict_data, fp) In\u00a0[\u00a0]: Copied! <pre># Predict informal data\npredict_data = {}\nfor temp in data_informal:\n    for id in data_informal[temp]:\n        # _, result = call_llm(data_informal[temp][id]) # Before finetune\n        _, result = call_llm_model(data_informal[temp][id]) # After finetune\n        try:\n            if not isinstance(result, dict):\n                predict_data[id] = eval(result)\n            else:\n                predict_data[id] = result\n        except:\n            print(result)\n\n# with open('./dataset/predict_informal_script.pkl', 'wb') as fp:\n#     pickle.dump(predict_data, fp)\nwith open('./dataset/finetune_predict_informal_script.pkl', 'wb') as fp:\n    pickle.dump(predict_data, fp)\n</pre> # Predict informal data predict_data = {} for temp in data_informal:     for id in data_informal[temp]:         # _, result = call_llm(data_informal[temp][id]) # Before finetune         _, result = call_llm_model(data_informal[temp][id]) # After finetune         try:             if not isinstance(result, dict):                 predict_data[id] = eval(result)             else:                 predict_data[id] = result         except:             print(result)  # with open('./dataset/predict_informal_script.pkl', 'wb') as fp: #     pickle.dump(predict_data, fp) with open('./dataset/finetune_predict_informal_script.pkl', 'wb') as fp:     pickle.dump(predict_data, fp) In\u00a0[\u00a0]: Copied! <pre># Predict novel data\npredict_data = {}\nfor temp in data_novel:\n    for id in data_novel[temp]:\n        # _, result = call_llm(data_novel[temp][id]) # Before finetune\n        _, result = call_llm_model(data_novel[temp][id]) # After finetune\n        try:\n            if not isinstance(result, dict):\n                predict_data[id] = eval(result)\n            else:\n                predict_data[id] = result\n        except:\n            print(result)\n\n# with open('./dataset/predict_novel_script.pkl', 'wb') as fp:\n#     pickle.dump(predict_data, fp)\nwith open('./dataset/finetune_predict_novel_script.pkl', 'wb') as fp:\n    pickle.dump(predict_data, fp)\n</pre> # Predict novel data predict_data = {} for temp in data_novel:     for id in data_novel[temp]:         # _, result = call_llm(data_novel[temp][id]) # Before finetune         _, result = call_llm_model(data_novel[temp][id]) # After finetune         try:             if not isinstance(result, dict):                 predict_data[id] = eval(result)             else:                 predict_data[id] = result         except:             print(result)  # with open('./dataset/predict_novel_script.pkl', 'wb') as fp: #     pickle.dump(predict_data, fp) with open('./dataset/finetune_predict_novel_script.pkl', 'wb') as fp:     pickle.dump(predict_data, fp) In\u00a0[\u00a0]: Copied! <pre>with open('./dataset/predict_formal_script.pkl', 'rb') as fp:\n    predict_formal = pickle.load(fp)\n\nwith open('./dataset/predict_informal_script.pkl', 'rb') as fp:\n    predict_informal = pickle.load(fp)\n\nwith open('./dataset/predict_novel_script.pkl', 'rb') as fp:\n    predict_novel = pickle.load(fp)\n\nwith open('./dataset/finetune_predict_formal_script.pkl', 'rb') as fp:\n    finetune_predict_formal = pickle.load(fp)\n\nwith open('./dataset/finetune_predict_informal_script.pkl', 'rb') as fp:\n    finetune_predict_informal = pickle.load(fp)\n\nwith open('./dataset/finetune_predict_novel_script.pkl', 'rb') as fp:\n    finetune_predict_novel = pickle.load(fp)\n\nwith open('./dataset/full_formal_script_temp0.8.pkl', 'rb') as fp:\n    data_formal = pickle.load(fp)\n\nwith open('./dataset/full_informal_script_temp0.8.pkl', 'rb') as fp:\n    data_informal = pickle.load(fp)\n\nwith open('./dataset/full_novel_script_temp0.8.pkl', 'rb') as fp:\n    data_novel = pickle.load(fp)\n\ndf = pd.read_csv('./dataset/raw.csv', index_col='uid')\n</pre> with open('./dataset/predict_formal_script.pkl', 'rb') as fp:     predict_formal = pickle.load(fp)  with open('./dataset/predict_informal_script.pkl', 'rb') as fp:     predict_informal = pickle.load(fp)  with open('./dataset/predict_novel_script.pkl', 'rb') as fp:     predict_novel = pickle.load(fp)  with open('./dataset/finetune_predict_formal_script.pkl', 'rb') as fp:     finetune_predict_formal = pickle.load(fp)  with open('./dataset/finetune_predict_informal_script.pkl', 'rb') as fp:     finetune_predict_informal = pickle.load(fp)  with open('./dataset/finetune_predict_novel_script.pkl', 'rb') as fp:     finetune_predict_novel = pickle.load(fp)  with open('./dataset/full_formal_script_temp0.8.pkl', 'rb') as fp:     data_formal = pickle.load(fp)  with open('./dataset/full_informal_script_temp0.8.pkl', 'rb') as fp:     data_informal = pickle.load(fp)  with open('./dataset/full_novel_script_temp0.8.pkl', 'rb') as fp:     data_novel = pickle.load(fp)  df = pd.read_csv('./dataset/raw.csv', index_col='uid') In\u00a0[\u00a0]: Copied! <pre>datasets = ['formal', 'informal', 'novel']\nfeatures = ['name', 'age', 'job']\nfor dataset in datasets:\n    for feature in features:\n        key = f\"pred_{dataset}_{feature}\"\n        key = f\"finetune_pred_{dataset}_{feature}\"\n        df[key] = None\n</pre> datasets = ['formal', 'informal', 'novel'] features = ['name', 'age', 'job'] for dataset in datasets:     for feature in features:         key = f\"pred_{dataset}_{feature}\"         key = f\"finetune_pred_{dataset}_{feature}\"         df[key] = None In\u00a0[\u00a0]: Copied! <pre>for i in finetune_predict_formal:\n    dataset = 'formal'\n    for feature in features:\n        key = f\"finetune_pred_{dataset}_{feature}\"\n        try:\n            if isinstance(finetune_predict_formal[i][feature], list):\n                df.loc[i, key] =  \" \".join(finetune_predict_formal[i][feature])\n            else:\n                df.loc[i, key] =  finetune_predict_formal[i][feature]\n        except Exception as e:\n            print(f\"{e}\")\n\nfor i in finetune_predict_informal:\n    dataset = 'informal'\n    for feature in features:\n        key = f\"finetune_pred_{dataset}_{feature}\"\n        try:\n            if isinstance(finetune_predict_informal[i][feature], list):\n                df.loc[i, key] =  \" \".join(finetune_predict_informal[i][feature])\n            else:\n                df.loc[i, key] =  finetune_predict_informal[i][feature]\n        except Exception as e:\n            print(f\"{e}\")\n\nfor i in finetune_predict_novel:\n    dataset = 'novel'\n    for feature in features:\n        key = f\"finetune_pred_{dataset}_{feature}\"\n        try:\n            if isinstance(finetune_predict_novel[i][feature], list):\n                df.loc[i, key] =  \" \".join(finetune_predict_novel[i][feature])\n            else:\n                df.loc[i, key] =  finetune_predict_novel[i][feature]\n        except Exception as e:\n            print(f\"{e}\")\n</pre> for i in finetune_predict_formal:     dataset = 'formal'     for feature in features:         key = f\"finetune_pred_{dataset}_{feature}\"         try:             if isinstance(finetune_predict_formal[i][feature], list):                 df.loc[i, key] =  \" \".join(finetune_predict_formal[i][feature])             else:                 df.loc[i, key] =  finetune_predict_formal[i][feature]         except Exception as e:             print(f\"{e}\")  for i in finetune_predict_informal:     dataset = 'informal'     for feature in features:         key = f\"finetune_pred_{dataset}_{feature}\"         try:             if isinstance(finetune_predict_informal[i][feature], list):                 df.loc[i, key] =  \" \".join(finetune_predict_informal[i][feature])             else:                 df.loc[i, key] =  finetune_predict_informal[i][feature]         except Exception as e:             print(f\"{e}\")  for i in finetune_predict_novel:     dataset = 'novel'     for feature in features:         key = f\"finetune_pred_{dataset}_{feature}\"         try:             if isinstance(finetune_predict_novel[i][feature], list):                 df.loc[i, key] =  \" \".join(finetune_predict_novel[i][feature])             else:                 df.loc[i, key] =  finetune_predict_novel[i][feature]         except Exception as e:             print(f\"{e}\") In\u00a0[\u00a0]: Copied! <pre>for i in predict_formal:\n    dataset = 'formal'\n    for feature in features:\n        key = f\"pred_{dataset}_{feature}\"\n        try:\n            if isinstance(predict_formal[i][feature], list):\n                df.loc[i, key] =  \" \".join(predict_formal[i][feature])\n            else:\n                df.loc[i, key] =  predict_formal[i][feature]\n        except Exception as e:\n            print(f\"{e}\")\n\nfor i in predict_informal:\n    dataset = 'informal'\n    for feature in features:\n        key = f\"pred_{dataset}_{feature}\"\n        try:\n            if isinstance(predict_informal[i][feature], list):\n                df.loc[i, key] =  \" \".join(predict_informal[i][feature])\n            else:\n                df.loc[i, key] =  predict_informal[i][feature]\n        except Exception as e:\n            print(f\"{e}\")\n\nfor i in predict_novel:\n    dataset = 'novel'\n    for feature in features:\n        key = f\"pred_{dataset}_{feature}\"\n        try:\n            if isinstance(predict_novel[i][feature], list):\n                df.loc[i, key] =  \" \".join(predict_novel[i][feature])\n            else:\n                df.loc[i, key] =  predict_novel[i][feature]\n        except Exception as e:\n            print(f\"{e}\")\n</pre> for i in predict_formal:     dataset = 'formal'     for feature in features:         key = f\"pred_{dataset}_{feature}\"         try:             if isinstance(predict_formal[i][feature], list):                 df.loc[i, key] =  \" \".join(predict_formal[i][feature])             else:                 df.loc[i, key] =  predict_formal[i][feature]         except Exception as e:             print(f\"{e}\")  for i in predict_informal:     dataset = 'informal'     for feature in features:         key = f\"pred_{dataset}_{feature}\"         try:             if isinstance(predict_informal[i][feature], list):                 df.loc[i, key] =  \" \".join(predict_informal[i][feature])             else:                 df.loc[i, key] =  predict_informal[i][feature]         except Exception as e:             print(f\"{e}\")  for i in predict_novel:     dataset = 'novel'     for feature in features:         key = f\"pred_{dataset}_{feature}\"         try:             if isinstance(predict_novel[i][feature], list):                 df.loc[i, key] =  \" \".join(predict_novel[i][feature])             else:                 df.loc[i, key] =  predict_novel[i][feature]         except Exception as e:             print(f\"{e}\") In\u00a0[\u00a0]: Copied! <pre>for dataset in datasets:\n    for feature in features:\n        key = f\"pred_{dataset}_{feature}\"\n        score_key = f\"pred_{dataset}_{feature}_score\"\n        df[score_key] = df[feature] == df[key]\n        \n        key = f\"finetune_pred_{dataset}_{feature}\"\n        score_key = f\"finetune_pred_{dataset}_{feature}_score\"\n        df[score_key] = df[feature] == df[key]\n</pre> for dataset in datasets:     for feature in features:         key = f\"pred_{dataset}_{feature}\"         score_key = f\"pred_{dataset}_{feature}_score\"         df[score_key] = df[feature] == df[key]                  key = f\"finetune_pred_{dataset}_{feature}\"         score_key = f\"finetune_pred_{dataset}_{feature}_score\"         df[score_key] = df[feature] == df[key] In\u00a0[\u00a0]: Copied! <pre>lenght = len(df)\nfor dataset in datasets:\n    print(f\"-------- {dataset.capitalize()} --------\")\n    for feature in features:\n        score_key = f\"pred_{dataset}_{feature}_score\"\n        print(f\"{feature.capitalize()} accuracy: {sum(df[score_key]) / len(df)}\")\n    print(f\"========================================\\n\")\n\n\nfor dataset in datasets:\n    print(f\"-------- Finetune - {dataset.capitalize()} --------\")\n    for feature in features:\n        score_key = f\"finetune_pred_{dataset}_{feature}_score\"\n        print(f\"{feature.capitalize()} accuracy: {sum(df[score_key]) / len(df)}\")\n    print(f\"========================================\\n\")\n</pre> lenght = len(df) for dataset in datasets:     print(f\"-------- {dataset.capitalize()} --------\")     for feature in features:         score_key = f\"pred_{dataset}_{feature}_score\"         print(f\"{feature.capitalize()} accuracy: {sum(df[score_key]) / len(df)}\")     print(f\"========================================\\n\")   for dataset in datasets:     print(f\"-------- Finetune - {dataset.capitalize()} --------\")     for feature in features:         score_key = f\"finetune_pred_{dataset}_{feature}_score\"         print(f\"{feature.capitalize()} accuracy: {sum(df[score_key]) / len(df)}\")     print(f\"========================================\\n\") In\u00a0[\u00a0]: Copied! <pre># -------- Formal --------\n# Name accuracy: 0.8715\n# Age accuracy: 0.959\n# Job accuracy: 0.612\n# ========================================\n\n# -------- Informal --------\n# Name accuracy: 0.8985\n# Age accuracy: 0.989\n# Job accuracy: 0.686\n# ========================================\n\n# -------- Novel --------\n# Name accuracy: 0.882\n# Age accuracy: 0.95\n# Job accuracy: 0.5505\n# ========================================\n\n# -------- Finetune - Formal --------\n# Name accuracy: 0.984\n# Age accuracy: 0.984\n# Job accuracy: 0.9995\n# ========================================\n\n# -------- Finetune - Informal --------\n# Name accuracy: 0.9775\n# Age accuracy: 0.9985\n# Job accuracy: 1.0\n# ========================================\n\n# -------- Finetune - Novel --------\n# Name accuracy: 0.949\n# Age accuracy: 0.9595\n# Job accuracy: 0.9885\n# ========================================\n</pre> # -------- Formal -------- # Name accuracy: 0.8715 # Age accuracy: 0.959 # Job accuracy: 0.612 # ========================================  # -------- Informal -------- # Name accuracy: 0.8985 # Age accuracy: 0.989 # Job accuracy: 0.686 # ========================================  # -------- Novel -------- # Name accuracy: 0.882 # Age accuracy: 0.95 # Job accuracy: 0.5505 # ========================================  # -------- Finetune - Formal -------- # Name accuracy: 0.984 # Age accuracy: 0.984 # Job accuracy: 0.9995 # ========================================  # -------- Finetune - Informal -------- # Name accuracy: 0.9775 # Age accuracy: 0.9985 # Job accuracy: 1.0 # ========================================  # -------- Finetune - Novel -------- # Name accuracy: 0.949 # Age accuracy: 0.9595 # Job accuracy: 0.9885 # ======================================== In\u00a0[\u00a0]: Copied! <pre>lenght = len(df)\ntest_df = df[1800:]\nfor dataset in datasets:\n    print(f\"-------- Test dataset - {dataset.capitalize()} --------\")\n    for feature in features:\n        score_key = f\"pred_{dataset}_{feature}_score\"\n        print(f\"{feature.capitalize()} accuracy: {sum(test_df[score_key]) / len(test_df)}\")\n    print(f\"========================================\\n\")\n\n\nfor dataset in datasets:\n    print(f\"-------- Finetune test dataset - {dataset.capitalize()} --------\")\n    for feature in features:\n        score_key = f\"finetune_pred_{dataset}_{feature}_score\"\n        print(f\"{feature.capitalize()} accuracy: {sum(test_df[score_key]) / len(test_df)}\")\n    print(f\"========================================\\n\")\n</pre> lenght = len(df) test_df = df[1800:] for dataset in datasets:     print(f\"-------- Test dataset - {dataset.capitalize()} --------\")     for feature in features:         score_key = f\"pred_{dataset}_{feature}_score\"         print(f\"{feature.capitalize()} accuracy: {sum(test_df[score_key]) / len(test_df)}\")     print(f\"========================================\\n\")   for dataset in datasets:     print(f\"-------- Finetune test dataset - {dataset.capitalize()} --------\")     for feature in features:         score_key = f\"finetune_pred_{dataset}_{feature}_score\"         print(f\"{feature.capitalize()} accuracy: {sum(test_df[score_key]) / len(test_df)}\")     print(f\"========================================\\n\") In\u00a0[\u00a0]: Copied! <pre># -------- Test dataset - Formal --------\n# Name accuracy: 0.87\n# Age accuracy: 0.965\n# Job accuracy: 0.685\n# ========================================\n\n# -------- Test dataset - Informal --------\n# Name accuracy: 0.895\n# Age accuracy: 0.98\n# Job accuracy: 0.75\n# ========================================\n\n# -------- Test dataset - Novel --------\n# Name accuracy: 0.875\n# Age accuracy: 0.935\n# Job accuracy: 0.55\n# ========================================\n\n# -------- Finetune test dataset - Formal --------\n# Name accuracy: 0.965\n# Age accuracy: 0.975\n# Job accuracy: 1.0\n# ========================================\n\n# -------- Finetune test dataset - Informal --------\n# Name accuracy: 0.96\n# Age accuracy: 0.995\n# Job accuracy: 1.0\n# ========================================\n\n# -------- Finetune test dataset - Novel --------\n# Name accuracy: 0.925\n# Age accuracy: 0.93\n# Job accuracy: 0.955\n# ========================================\n</pre> # -------- Test dataset - Formal -------- # Name accuracy: 0.87 # Age accuracy: 0.965 # Job accuracy: 0.685 # ========================================  # -------- Test dataset - Informal -------- # Name accuracy: 0.895 # Age accuracy: 0.98 # Job accuracy: 0.75 # ========================================  # -------- Test dataset - Novel -------- # Name accuracy: 0.875 # Age accuracy: 0.935 # Job accuracy: 0.55 # ========================================  # -------- Finetune test dataset - Formal -------- # Name accuracy: 0.965 # Age accuracy: 0.975 # Job accuracy: 1.0 # ========================================  # -------- Finetune test dataset - Informal -------- # Name accuracy: 0.96 # Age accuracy: 0.995 # Job accuracy: 1.0 # ========================================  # -------- Finetune test dataset - Novel -------- # Name accuracy: 0.925 # Age accuracy: 0.93 # Job accuracy: 0.955 # ========================================"},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/04.Evaluate/#set-up","title":"Set up\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/04.Evaluate/#load-model","title":"Load model\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/04.Evaluate/#load-finetuned-model","title":"Load finetuned model\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/04.Evaluate/#predict","title":"Predict\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/04.Evaluate/#evaluate","title":"Evaluate\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/04.Evaluate/#calculate-score","title":"Calculate score\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/04.Evaluate/#evaluate-finetune-model","title":"Evaluate finetune model\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/04.Evaluate/#evaluate-base-model","title":"Evaluate base model\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/04.Evaluate/#calculate-all-dataset","title":"Calculate all dataset\u00b6","text":""},{"location":"knowledge-sharing/5%20-%20Optimizing%20LLM%20with%20LoRA/material/fintune-llm-with-lora/04.Evaluate/#calculate-only-test-dataset","title":"Calculate only test dataset\u00b6","text":""}]}