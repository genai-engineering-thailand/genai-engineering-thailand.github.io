{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#join-our-community","title":"Join our community","text":"<p>Let's connect and make our community stay alive. Please read our contribution guidelines about detailed workflow. For general questions or updates, public discussions and event appointments, please join our community.</p> <p> </p> <p>All contributions are greatly appreciated </p>"},{"location":"awesome_curation/","title":"Content Curations","text":""},{"location":"awesome_curation/#awesome-curations","title":"Awesome Curations","text":"<p>The following list consists of awesome portals related to LLM and Generative AI in various aspects.</p> <ul> <li>Awesome-LLM</li> <li>LLMsPracticalGuide</li> </ul>"},{"location":"knowledge-sharing/content_index/","title":"Content Index","text":""},{"location":"knowledge-sharing/content_index/#knowledge-sharing","title":"Knowledge Sharing","text":"Date Knowledge Content 17 May 2024 Self-hosted LLM on GCP"},{"location":"knowledge-sharing/0%20-%20Self-hosted%20LLM%20on%20GCP/","title":"Self-hosted LLM on GCP","text":""},{"location":"knowledge-sharing/0%20-%20Self-hosted%20LLM%20on%20GCP/#resource","title":"Resource","text":"<p>Video | Material | Knowledge Summary</p>"},{"location":"knowledge-sharing/0%20-%20Self-hosted%20LLM%20on%20GCP/#summary","title":"Summary","text":"<p>The following content is summarized by Gemini.</p> <p>Topic 1: Introduction</p> <ul> <li>This is the first event hosted by GenAI Engineer Thailand, a community for people who are interested in generative AI.</li> <li>The goal of this event is to share knowledge and learn from each other.</li> <li>The speaker for this event is Mr. Coco - Senior AI/ML Engineer from ArcFusion.ai</li> </ul> <p>Topic 2: LLM deployment on GCP</p> <ul> <li>Large Language Model (LLM) inference is memory bound, not compute bound. This means it takes longer to load data into GPU memory than it does to process the data itself. </li> <li>Because of this, the bottleneck for LLM inference is the size of the model that can fit into GPU memory.</li> <li>Techniques to reduce memory usage and speed up inference include quantization, like AutoGPTQ (reducing the representation size of data) or FlashAttention (modifying the Attention mechanism).</li> <li>Ray Serve and vLLM are frameworks that can be used for LLM deployment. </li> <li>Ray and vLLM are tools used together to run large language models (LLMs) efficiently:<ul> <li>Ray:<ul> <li>Distributed computing framework</li> <li>Manages resources across multiple machines</li> <li>Enables parallel processing and scaling</li> </ul> </li> <li>vLLM:<ul> <li>Open-source LLM inference engine</li> <li>Optimizes memory usage for faster inference</li> </ul> </li> </ul> </li> </ul> <p>Together, Ray and vLLM provide a powerful solution for deploying and scaling LLMs in production environments. They are often used to build high-performance LLM serving systems that can handle large volumes of requests.</p>"},{"location":"knowledge-sharing/0%20-%20Self-hosted%20LLM%20on%20GCP/material/content/","title":"content","text":""},{"location":"knowledge-sharing/0%20-%20Self-hosted%20LLM%20on%20GCP/material/content/#files","title":"Files","text":"<ul> <li>PDF</li> </ul>"},{"location":"knowledge-sharing/1%20-%20PDF-to-Text%20-%20A%20nightmare%20that%20never%20ends/","title":"PDF-to-Text: A nightmare that never ends","text":""},{"location":"knowledge-sharing/1%20-%20PDF-to-Text%20-%20A%20nightmare%20that%20never%20ends/#resource","title":"Resource","text":"<p>Video | Material | Knowledge Summary</p>"},{"location":"knowledge-sharing/1%20-%20PDF-to-Text%20-%20A%20nightmare%20that%20never%20ends/#summary","title":"Summary","text":"<p>The video is about PDF-to-text and the challenges involved. The speaker, Napat, an AI/ML engineer at ArcFusion.ai who has experience working on PDF-to-Text conversion, discusses the difficulties of converting PDFs to text, especially scanned PDFs.</p> <p>The speaker mentions that there are 3 main components to consider when working with PDFs: images, tables, and text.</p> <ul> <li>Images: Extracting images from digital PDFs is straightforward, but extracting images from scanned PDFs requires using object detection.</li> <li>Tables: Extracting tables can be done using libraries if the tables have borders around each cell. Extracting tables without borders is more challenging.</li> <li>Text: Text extraction is the most important part. The speaker mentions that there are challenges with accuracy, especially when dealing with scanned PDFs, which needed OCR.</li> </ul>"},{"location":"knowledge-sharing/1%20-%20PDF-to-Text%20-%20A%20nightmare%20that%20never%20ends/material/content/","title":"content","text":""},{"location":"knowledge-sharing/1%20-%20PDF-to-Text%20-%20A%20nightmare%20that%20never%20ends/material/content/#files","title":"Files","text":"<ul> <li>PDF</li> </ul>"},{"location":"knowledge-sharing/2%20-%20Introduction%20to%20Ollama/","title":"Introduction to Ollama","text":""},{"location":"knowledge-sharing/2%20-%20Introduction%20to%20Ollama/#resource","title":"Resource","text":"<p>Video | Material | Knowledge Summary</p>"},{"location":"knowledge-sharing/2%20-%20Introduction%20to%20Ollama/#summary","title":"Summary","text":"<p>The following content is summarized by Gemini.</p> <p>Introduction</p> <p>The speaker introduces himself as Pako, a Tech guy from PALO IT. He is excited to share what he learned about Ollama.</p> <p>Why run LLM or SLM locally:</p> <p>The speaker discusses why one would want to run LLM or SLM locally when there are already commercial, well-known servers available. Reasons for running locally include:</p> <ul> <li>Developer can learn and experiment with LM concepts more easily on their own machine without any cost.</li> <li>It helps improve coding skills in writing prompts as the local LM is not as powerful as mainstream models.</li> </ul> <p>What is Ollama:</p> <p>Ollama is a wrapper for llama.cpp. It allows you to run LLM on your local CPU or laptop. Ollama converts the model to a format that can be run on the CPU. It is a powerful tool that can run various things including converting models. Ollama improves developer experience by allowing them to run LLM and serve as an APIs. This means you can use large language models through APIs. Ollama becomes versatile and can be used in Docker, Kubernetes environments, etc. It can also be called through NodeJS, Python, etc. Ollama comes with good community support on Discord.</p> <p>Background of Ollama:</p> <p>The speaker talks about the background of Ollama. Since the developers of Ollama previously worked on Docker, the command line interface (CLI) is similar to Docker. For instance, commands like push model, pull model, and ama run are similar to Docker commands.</p> <p>How to use OLM:</p> <p>Ollama is not necessarily the fastest because it runs on your local machine. Be patient when running models as it depends on your CPU. Ollama itself is not a model, it runs models that are converted to gguf format. Converting models to gguf is not difficult and there are tools available including Ollama itself. There are instructions and libraries to convert various model formats to gguf.</p> <p>The speaker walks through downloading and running Ollama from the Github page. He shows that Ollama can list models available on the machine and you can see the configuration by running the show config command. Ollama also has templates which are ready-made prompts that can be used. It can also run LoRA adapters.</p> <p>What's Next</p> <p>The speaker suggests exploring Kubernetes with Ollama to run models on Kubernetes. He also recommends exploring creating custom models and using Ollama with libraries like Tensorflow or PyTorch. Finally, he suggests trying out fine-tuning models from UnSloth and using them in Ollama.</p>"},{"location":"knowledge-sharing/2%20-%20Introduction%20to%20Ollama/material/content/","title":"content","text":""},{"location":"knowledge-sharing/2%20-%20Introduction%20to%20Ollama/material/content/#files","title":"Files","text":"<ul> <li>PDF</li> </ul>"}]}