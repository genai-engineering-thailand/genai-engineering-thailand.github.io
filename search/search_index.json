{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#join-our-community","title":"Join our community","text":"<p>Let's connect and make our community stay alive. Please read our contribution guidelines about detailed workflow. For general questions or updates, public discussions and event appointments, please join our community.</p> <p> </p> <p>All contributions are greatly appreciated </p>"},{"location":"awesome_curation/","title":"Content Curations","text":""},{"location":"awesome_curation/#awesome-curations","title":"Awesome Curations","text":"<p>The following list consists of awesome portals related to LLM and Generative AI in various aspects.</p> <ul> <li>Awesome-LLM</li> <li>LLMsPracticalGuide</li> </ul>"},{"location":"knowledge-sharing/content_index/","title":"Content Index","text":""},{"location":"knowledge-sharing/content_index/#knowledge-sharing","title":"Knowledge Sharing","text":"Date Knowledge Content 17 May 2024 Self-hosted LLM on GCP"},{"location":"knowledge-sharing/0%20-%20Self-hosted%20LLM%20on%20GCP/","title":"Self-hosted LLM on GCP","text":""},{"location":"knowledge-sharing/0%20-%20Self-hosted%20LLM%20on%20GCP/#resource","title":"Resource","text":"<p>Video | Material | Knowledge Summary</p>"},{"location":"knowledge-sharing/0%20-%20Self-hosted%20LLM%20on%20GCP/#summary","title":"Summary","text":"<p>The following content is summarized by Gemini.</p> <p>Topic 1: Introduction</p> <ul> <li>This is the first event hosted by GenAI Engineer Thailand, a community for people who are interested in generative AI.</li> <li>The goal of this event is to share knowledge and learn from each other.</li> <li>The speaker for this event is Mr. Coco - Senior AI/ML Engineer from ArcFusion.ai</li> </ul> <p>Topic 2: LLM deployment on GCP</p> <ul> <li>Large Language Model (LLM) inference is memory bound, not compute bound. This means it takes longer to load data into GPU memory than it does to process the data itself. </li> <li>Because of this, the bottleneck for LLM inference is the size of the model that can fit into GPU memory.</li> <li>Techniques to reduce memory usage and speed up inference include quantization, like AutoGPTQ (reducing the representation size of data) or FlashAttention (modifying the Attention mechanism).</li> <li>Ray Serve and vLLM are frameworks that can be used for LLM deployment. </li> <li>Ray and vLLM are tools used together to run large language models (LLMs) efficiently:<ul> <li>Ray:<ul> <li>Distributed computing framework</li> <li>Manages resources across multiple machines</li> <li>Enables parallel processing and scaling</li> </ul> </li> <li>vLLM:<ul> <li>Open-source LLM inference engine</li> <li>Optimizes memory usage for faster inference</li> </ul> </li> </ul> </li> </ul> <p>Together, Ray and vLLM provide a powerful solution for deploying and scaling LLMs in production environments. They are often used to build high-performance LLM serving systems that can handle large volumes of requests.</p>"},{"location":"knowledge-sharing/0%20-%20Self-hosted%20LLM%20on%20GCP/material/content/","title":"content","text":""},{"location":"knowledge-sharing/0%20-%20Self-hosted%20LLM%20on%20GCP/material/content/#files","title":"Files","text":"<ul> <li>PDF</li> </ul>"},{"location":"knowledge-sharing/1%20-%20PDF-to-Text%20-%20A%20nightmare%20that%20never%20ends/","title":"PDF-to-Text: A nightmare that never ends","text":""},{"location":"knowledge-sharing/1%20-%20PDF-to-Text%20-%20A%20nightmare%20that%20never%20ends/#resource","title":"Resource","text":"<p>Video | Material | Knowledge Summary</p>"},{"location":"knowledge-sharing/1%20-%20PDF-to-Text%20-%20A%20nightmare%20that%20never%20ends/#summary","title":"Summary","text":"<p>The video is about PDF-to-text and the challenges involved. The speaker, Napat, an AI/ML engineer at ArcFusion.ai who has experience working on PDF-to-Text conversion, discusses the difficulties of converting PDFs to text, especially scanned PDFs.</p> <p>The speaker mentions that there are 3 main components to consider when working with PDFs: images, tables, and text.</p> <ul> <li>Images: Extracting images from digital PDFs is straightforward, but extracting images from scanned PDFs requires using object detection.</li> <li>Tables: Extracting tables can be done using libraries if the tables have borders around each cell. Extracting tables without borders is more challenging.</li> <li>Text: Text extraction is the most important part. The speaker mentions that there are challenges with accuracy, especially when dealing with scanned PDFs, which needed OCR.</li> </ul>"},{"location":"knowledge-sharing/1%20-%20PDF-to-Text%20-%20A%20nightmare%20that%20never%20ends/material/content/","title":"content","text":""},{"location":"knowledge-sharing/1%20-%20PDF-to-Text%20-%20A%20nightmare%20that%20never%20ends/material/content/#files","title":"Files","text":"<ul> <li>PDF</li> </ul>"}]}